{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Input \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import csv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#TODO fix saliency https://github.com/raghakot/keras-vis/issues/228\n",
    "from vis.visualization import visualize_saliency\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "import scipy.ndimage as ndimage\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "import keras\n",
    "#from keras.layers.core import Layer\n",
    "import keras.layers.core\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPool2D,  \\\n",
    "    Dropout, Dense, Input, concatenate,      \\\n",
    "    GlobalAveragePooling2D, AveragePooling2D,\\\n",
    "    Flatten\n",
    "\n",
    "import cv2 \n",
    "import numpy as np \n",
    "from keras.datasets import cifar10 \n",
    "from keras import backend as K \n",
    "from keras.utils import np_utils\n",
    "\n",
    "import math \n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category = FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "#hooking up GPU\n",
    "physical_dev = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs avail: \", len(physical_dev))\n",
    "tf.config.experimental.set_memory_growth(physical_dev[0], True)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "\n",
    "os.chdir('C:/venv/tensorflow-scripting/data/lung-scans')\n",
    "\n",
    "CURR_DIR = os.getcwd()\n",
    "\n",
    "'''\n",
    "#Grayscale \n",
    "if os.path.isdir('train_images_grays') is False:\n",
    "    os.makedirs('train_images_grays')\n",
    "    with open('labels/labels_train.csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            img = Image.open(CURR_DIR + '/train_images/' + row['file_name'])\n",
    "            img2 = img.convert('L') \n",
    "            img2.save(CURR_DIR + '/train_images_grays/' + row['file_name'])\n",
    "'''   \n",
    "\n",
    "\n",
    "\n",
    "#Data org\n",
    "#Keras's ImageDataGenerator.flow_from_directory() labels input based on folder structure. \n",
    "#Hence we are going to organize train, valid and test data with their respective classes healthy, bacterial and viral \n",
    "if os.path.isdir('train/healthy') is False:\n",
    "    os.makedirs('train/healthy')\n",
    "    os.makedirs('train/bacterial')\n",
    "    os.makedirs('train/viral')\n",
    "    os.makedirs('valid/healthy')\n",
    "    os.makedirs('valid/bacterial')\n",
    "    os.makedirs('valid/viral')\n",
    "    os.makedirs('test/healthy')\n",
    "    os.makedirs('test/bacterial')\n",
    "    os.makedirs('test/viral')\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    class_id = 0 if the image corresponds to a subject without disease (normal)  1227\n",
    "    \n",
    "    class_id = 1 if the image corresponds to a patient with bacterial pneumonia  2238\n",
    "\n",
    "    class_id = 2 if the image corresponds to a patient with viral pneumonia      1207\n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #purely cause I don't won't to rename it if I use greyscale folder\n",
    "    train_images_folder = '/train_images/'\n",
    "    \n",
    "    #looking through the doc of labels and copying images to respetictive folders\n",
    "    with open('labels/labels_train.csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        class_id_0 = 0\n",
    "        class_id_1 = 0\n",
    "        class_id_2 = 0\n",
    "        for row in reader:\n",
    "#HEALTHY\n",
    "            if(row['class_id'] == '0'):\n",
    "                class_id_0+=1\n",
    "                if(len([name for name in os.listdir(CURR_DIR + '/train/healthy/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/train/healthy/', name))]) < 1077):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/train/healthy/')\n",
    "                elif(len([name for name in os.listdir(CURR_DIR + '/valid/healthy/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/valid/healthy/', name))]) < 100):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/valid/healthy/')\n",
    "                elif(len([name for name in os.listdir(CURR_DIR + '/test/healthy/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/test/healthy/', name))]) < 50):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/test/healthy/')\n",
    "                    \n",
    "#BACTERIAL\n",
    "            if(row['class_id'] == '1'):\n",
    "                if(len([name for name in os.listdir(CURR_DIR + '/train/bacterial/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/train/bacterial/', name))]) < 2088):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/train/bacterial/')\n",
    "                elif(len([name for name in os.listdir(CURR_DIR + '/valid/bacterial/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/valid/bacterial/', name))]) < 100):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/valid/bacterial/')\n",
    "                elif(len([name for name in os.listdir(CURR_DIR + '/test/bacterial/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/test/bacterial/', name))]) < 50):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/test/bacterial/')\n",
    "                    \n",
    "#VIRAL                        \n",
    "            if(row['class_id'] == '2'):\n",
    "                if(len([name for name in os.listdir(CURR_DIR + '/train/viral/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/train/viral/', name))]) < 1057):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/train/viral/')\n",
    "                elif(len([name for name in os.listdir(CURR_DIR + '/valid/viral/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/valid/viral/', name))]) < 100):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/valid/viral/')\n",
    "                elif(len([name for name in os.listdir(CURR_DIR + '/test/viral/')\n",
    "                    if os.path.isfile(os.path.join(CURR_DIR + '/test/viral/', name))]) < 50):\n",
    "                    file = train_images_folder + row['file_name']\n",
    "                    shutil.copy(CURR_DIR + file, CURR_DIR + '/test/viral/')\n",
    "                    \n",
    "\n",
    "os.chdir('../../')\n",
    "print('finished organising data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/lung-scans/train'\n",
    "valid_path = 'data/lung-scans/valid'\n",
    "test_path = 'data/lung-scans/test'\n",
    "\n",
    "# then use this function as the preprocessing function in the generator\n",
    "def grey_preprocessor (xarray):\n",
    "    xarray=(xarray/127.5)-1\n",
    "    return xarray\n",
    "\n",
    "img_resol = (224,224)\n",
    "train_batches = ImageDataGenerator(horizontal_flip = True, preprocessing_function = grey_preprocessor).flow_from_directory(\n",
    "    directory = train_path, target_size=img_resol, classes = ['bacterial', 'healthy', 'viral'], batch_size = 10)\n",
    "valid_batches = ImageDataGenerator(horizontal_flip = True,  preprocessing_function = grey_preprocessor).flow_from_directory(\n",
    "    directory = valid_path, target_size=img_resol, classes = ['bacterial', 'healthy', 'viral'], batch_size = 10)\n",
    "test_batches = ImageDataGenerator(horizontal_flip = True,  preprocessing_function = grey_preprocessor).flow_from_directory(\n",
    "    directory = test_path, target_size=img_resol, classes = ['bacterial', 'healthy', 'viral'], batch_size = 10, shuffle = False)\n",
    "\n",
    "assert train_batches.n == 4222\n",
    "assert valid_batches.n == 300\n",
    "assert test_batches.n == 150\n",
    "assert train_batches.num_classes == valid_batches.num_classes == test_batches.num_classes == 3\n",
    "\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize = (20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotImages(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "\n",
    "def inception_module(x,\n",
    "                     filters_1x1,\n",
    "                     filters_3x3_reduce,\n",
    "                     filters_3x3,\n",
    "                     filters_5x5_reduce,\n",
    "                     filters_5x5,\n",
    "                     filters_pool_proj,\n",
    "                     name=None):\n",
    "    \n",
    "    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
    "    \n",
    "    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
    "    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n",
    "\n",
    "    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
    "    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n",
    "\n",
    "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n",
    "\n",
    "    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n",
    "    \n",
    "    return output\n",
    "\n",
    "kernel_init = keras.initializers.glorot_uniform()\n",
    "bias_init = keras.initializers.Constant(value=0.2)\n",
    "\n",
    "input_layer = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)\n",
    "x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)\n",
    "x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=64,\n",
    "                     filters_3x3_reduce=96,\n",
    "                     filters_3x3=128,\n",
    "                     filters_5x5_reduce=16,\n",
    "                     filters_5x5=32,\n",
    "                     filters_pool_proj=32,\n",
    "                     name='inception_3a')\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=128,\n",
    "                     filters_3x3_reduce=128,\n",
    "                     filters_3x3=192,\n",
    "                     filters_5x5_reduce=32,\n",
    "                     filters_5x5=96,\n",
    "                     filters_pool_proj=64,\n",
    "                     name='inception_3b')\n",
    "\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=192,\n",
    "                     filters_3x3_reduce=96,\n",
    "                     filters_3x3=208,\n",
    "                     filters_5x5_reduce=16,\n",
    "                     filters_5x5=48,\n",
    "                     filters_pool_proj=64,\n",
    "                     name='inception_4a')\n",
    "\n",
    "\n",
    "x1 = AveragePooling2D((5, 5), strides=3)(x)\n",
    "x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(1024, activation='relu')(x1)\n",
    "x1 = Dropout(0.7)(x1)\n",
    "x1 = Dense(3, activation='softmax', name='auxilliary_output_1')(x1)\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=160,\n",
    "                     filters_3x3_reduce=112,\n",
    "                     filters_3x3=224,\n",
    "                     filters_5x5_reduce=24,\n",
    "                     filters_5x5=64,\n",
    "                     filters_pool_proj=64,\n",
    "                     name='inception_4b')\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=128,\n",
    "                     filters_3x3_reduce=128,\n",
    "                     filters_3x3=256,\n",
    "                     filters_5x5_reduce=24,\n",
    "                     filters_5x5=64,\n",
    "                     filters_pool_proj=64,\n",
    "                     name='inception_4c')\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=112,\n",
    "                     filters_3x3_reduce=144,\n",
    "                     filters_3x3=288,\n",
    "                     filters_5x5_reduce=32,\n",
    "                     filters_5x5=64,\n",
    "                     filters_pool_proj=64,\n",
    "                     name='inception_4d')\n",
    "\n",
    "\n",
    "x2 = AveragePooling2D((5, 5), strides=3)(x)\n",
    "x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(1024, activation='relu')(x2)\n",
    "x2 = Dropout(0.7)(x2)\n",
    "x2 = Dense(3, activation='softmax', name='auxilliary_output_2')(x2)\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=256,\n",
    "                     filters_3x3_reduce=160,\n",
    "                     filters_3x3=320,\n",
    "                     filters_5x5_reduce=32,\n",
    "                     filters_5x5=128,\n",
    "                     filters_pool_proj=128,\n",
    "                     name='inception_4e')\n",
    "\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=256,\n",
    "                     filters_3x3_reduce=160,\n",
    "                     filters_3x3=320,\n",
    "                     filters_5x5_reduce=32,\n",
    "                     filters_5x5=128,\n",
    "                     filters_pool_proj=128,\n",
    "                     name='inception_5a')\n",
    "\n",
    "x = inception_module(x,\n",
    "                     filters_1x1=384,\n",
    "                     filters_3x3_reduce=192,\n",
    "                     filters_3x3=384,\n",
    "                     filters_5x5_reduce=48,\n",
    "                     filters_5x5=128,\n",
    "                     filters_pool_proj=128,\n",
    "                     name='inception_5b')\n",
    "\n",
    "x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\n",
    "\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(3, activation='softmax', name='output')(x)\n",
    "\n",
    "model = Model(input_layer, [x, x1, x2], name='inception_v1')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 25\n",
    "initial_lrate = 0.01\n",
    "\n",
    "def decay(epoch, steps=100):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.96\n",
    "    epochs_drop = 8\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\n",
    "\n",
    "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer= sgd, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_batches, validation_data=valid_batches, epochs=epochs, batch_size=256, callbacks=[lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_imgs, test_labels = next(test_batches)\n",
    "plotImages(test_imgs)\n",
    "\n",
    "\n",
    "predictions = model.predict(x=test_batches, verbose = 0)\n",
    "\n",
    "test_batches.classes\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true = test_batches.classes, y_pred = np.argmax(predictions[2], axis = -1))\n",
    "test_batches.class_indices\n",
    "\n",
    "cm_plot_labels = ['bacterial', 'healthy', 'viral']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize = False,\n",
    "                          title = 'Cunfusion matrix',\n",
    "                          cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting 'normalize  = True'.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without norm\")\n",
    "        \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i,j],\n",
    "            horizontalalignment = \"center\",\n",
    "            color = \"white\" if cm[i,j] > thresh else \"black\")\n",
    "            \n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predict label')\n",
    "\n",
    "plot_confusion_matrix(cm = cm, classes = cm_plot_labels, title = 'Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['output_accuracy'])\n",
    "plt.plot(history.history['val_output_accuracy'])\n",
    "\n",
    "plt.title('model accuracy 1')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
